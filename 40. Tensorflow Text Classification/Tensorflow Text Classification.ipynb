{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification is the task of assigning the right label to a given piece of text. This text can either be a phrase, a sentence or even a paragraph. Our aim would be to take in some text as input and attach or assign a label to it. Since we will be using Tensor Flow Is deep learning library, we can call this the Tensorflow text classification system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task involves training a neural network with lots of data indicating what a piece of text represents. I am sure you would have heard of the term “Sentiment Analysis“. Well, sentiment analysis a text classification task but it is restricted only to identify the sentiment of the person saying something. For example, the sentence, ” The food was amazing” has a positive sentiment. On the other hand, ” the movie was horrible” has a negative sentiment while the sentence “sun rises from the east” has a neutral sentiment.\n",
    "\n",
    "For sentiment analysis, the labels are positive, negative and neutral most of the times. But, this is just one use of the text classification. If you are building other text-based applications like a chatbot, or a document parsing algorithm, you might want to know what a particular sentence belongs to. For example: ” Hello! how are you?” can have the label “Greeting” attached to it or the sentence ” It was a pleasure meeting you” can have the label “Farewell” attached to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we you going to do ?\n",
    "\n",
    "We are gonna build a simple text classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation\n",
    "\n",
    "Before we train a model that can classify a given text to a particular category, we have to first prepare the data. We can create a simple JSON file that will hold the required data for training.\n",
    "\n",
    "Following is a sample file that I have created, that contains 5 categories. You can create how many ever categories that you want.\n",
    "\n",
    "{\n",
    " \n",
    "\"time\" : [\"what time is it?\", \"how long has it been since we started?\", \"that's a long time ago\", \" I spoke to you last week\", \" I saw you yesterday\"],\n",
    " \n",
    "\"sorry\" : [\"I'm extremely sorry\", \"did he apologize to you?\", \"I shouldn't have been rude\"],\n",
    " \n",
    "\"greeting\": [\"Hello there!\", \"Hey man! How are you?\", \"hi\"],\n",
    " \n",
    "\"farewell\": [\"It was a pleasure meeting you\", \"Good Bye.\", \"see you soon\", \"I gotta go now.\"],\n",
    " \n",
    "\"age\": [\"what's your age?\", \"How old are you?\", \"I'm a couple of years older than her\", \"You look aged!\"]\n",
    " \n",
    "}\n",
    "\n",
    "\n",
    "In the above structure, we have a simple JSON with 5 categories ( time, sorry, greeting, farewell, and age). For each category, we have a set of sentences which we can use to train our model.\n",
    "\n",
    "Given this data, we have to classify any given sentence into one of these 5 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Load and Pre-processing\n",
    "\n",
    "In this step, we load the JSON data that we have created. Let us assume that we have that data stored in a file named “data.json”.\n",
    "\n",
    "Once we load the data, we would have to perform some operations on it to clean the data and form the bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "# a table structure to hold the different punctuation used\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                    if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': ['what time is it?', 'how long has it been since we started?', \"that's a long time ago\", ' I spoke to you last week', ' I saw you yesterday'], 'sorry': [\"I'm extremely sorry\", 'did he apologize to you?', \"I shouldn't have been rude\"], 'greeting': ['Hello there!', 'Hey man! How are you?', 'hi'], 'farewell': ['It was a pleasure meeting you', 'Good Bye.', 'see you soon', 'I gotta go now.'], 'age': [\"what's your age?\", 'How old are you?', \"I'm a couple of years older than her\", 'You look aged!']}\n",
      "what time is it\n",
      "tokenized words:  ['what', 'time', 'is', 'it']\n",
      "how long has it been since we started\n",
      "tokenized words:  ['how', 'long', 'has', 'it', 'been', 'since', 'we', 'started']\n",
      "thats a long time ago\n",
      "tokenized words:  ['thats', 'a', 'long', 'time', 'ago']\n",
      " I spoke to you last week\n",
      "tokenized words:  ['I', 'spoke', 'to', 'you', 'last', 'week']\n",
      " I saw you yesterday\n",
      "tokenized words:  ['I', 'saw', 'you', 'yesterday']\n",
      "Im extremely sorry\n",
      "tokenized words:  ['Im', 'extremely', 'sorry']\n",
      "did he apologize to you\n",
      "tokenized words:  ['did', 'he', 'apologize', 'to', 'you']\n",
      "I shouldnt have been rude\n",
      "tokenized words:  ['I', 'shouldnt', 'have', 'been', 'rude']\n",
      "Hello there\n",
      "tokenized words:  ['Hello', 'there']\n",
      "Hey man How are you\n",
      "tokenized words:  ['Hey', 'man', 'How', 'are', 'you']\n",
      "hi\n",
      "tokenized words:  ['hi']\n",
      "It was a pleasure meeting you\n",
      "tokenized words:  ['It', 'was', 'a', 'pleasure', 'meeting', 'you']\n",
      "Good Bye\n",
      "tokenized words:  ['Good', 'Bye']\n",
      "see you soon\n",
      "tokenized words:  ['see', 'you', 'soon']\n",
      "I gotta go now\n",
      "tokenized words:  ['I', 'got', 'ta', 'go', 'now']\n",
      "whats your age\n",
      "tokenized words:  ['whats', 'your', 'age']\n",
      "How old are you\n",
      "tokenized words:  ['How', 'old', 'are', 'you']\n",
      "Im a couple of years older than her\n",
      "tokenized words:  ['Im', 'a', 'couple', 'of', 'years', 'older', 'than', 'her']\n",
      "You look aged\n",
      "tokenized words:  ['You', 'look', 'aged']\n",
      "['a', 'ag', 'ago', 'apolog', 'ar', 'been', 'bye', 'coupl', 'did', 'extrem', 'go', 'good', 'got', 'has', 'hav', 'he', 'hello', 'her', 'hey', 'hi', 'how', 'i', 'im', 'is', 'it', 'last', 'long', 'look', 'man', 'meet', 'now', 'of', 'old', 'pleas', 'rud', 'saw', 'see', 'shouldnt', 'sint', 'soon', 'sorry', 'spok', 'start', 'ta', 'than', 'that', 'ther', 'tim', 'to', 'was', 'we', 'week', 'what', 'year', 'yesterday', 'yo', 'you']\n",
      "[(['what', 'time', 'is', 'it'], 'time'), (['how', 'long', 'has', 'it', 'been', 'since', 'we', 'started'], 'time'), (['thats', 'a', 'long', 'time', 'ago'], 'time'), (['I', 'spoke', 'to', 'you', 'last', 'week'], 'time'), (['I', 'saw', 'you', 'yesterday'], 'time'), (['Im', 'extremely', 'sorry'], 'sorry'), (['did', 'he', 'apologize', 'to', 'you'], 'sorry'), (['I', 'shouldnt', 'have', 'been', 'rude'], 'sorry'), (['Hello', 'there'], 'greeting'), (['Hey', 'man', 'How', 'are', 'you'], 'greeting'), (['hi'], 'greeting'), (['It', 'was', 'a', 'pleasure', 'meeting', 'you'], 'farewell'), (['Good', 'Bye'], 'farewell'), (['see', 'you', 'soon'], 'farewell'), (['I', 'got', 'ta', 'go', 'now'], 'farewell'), (['whats', 'your', 'age'], 'age'), (['How', 'old', 'are', 'you'], 'age'), (['Im', 'a', 'couple', 'of', 'years', 'older', 'than', 'her'], 'age'), (['You', 'look', 'aged'], 'age')]\n"
     ]
    }
   ],
   "source": [
    "# method to remove punctuations from sentences.\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "# initialize the stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# variable to hold the Json data read from the file\n",
    "data = None\n",
    "\n",
    "# read the json file and load the training data\n",
    "with open('data.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    print(data)\n",
    "\n",
    "# get a list of all categories to train for\n",
    "categories = list(data.keys())\n",
    "words = []\n",
    "# a list of tuples with words in the sentence and category name\n",
    "docs = []\n",
    "\n",
    "for each_category in data.keys():\n",
    "    for each_sentence in data[each_category]:\n",
    "        # remove any punctuation from the sentence\n",
    "        each_sentence = remove_punctuation(each_sentence)\n",
    "        print(each_sentence)\n",
    "        # extract words from each sentence and append to the word list\n",
    "        w = nltk.word_tokenize(each_sentence)\n",
    "        print(\"tokenized words: \", w)\n",
    "        words.extend(w)\n",
    "        docs.append((w, each_category))\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "print(words)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    # initialize our bag of words(bow) for each document in the list\n",
    "    bow = []\n",
    "    # list of tokenized words for the pattern\n",
    "    token_words = doc[0]\n",
    "    # stem each word\n",
    "    token_words = [stemmer.stem(word.lower()) for word in token_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bow.append(1) if w in token_words else bow.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[categories.index(doc[1])] = 1\n",
    "\n",
    "    # our training set will contain a the bag of words model and the output row that tells\n",
    "    # which catefory that bow belongs to.\n",
    "    training.append([bow, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array as tensorflow  takes in numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# trainX contains the Bag of words and train_y contains the label/ category\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.19740\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1000 | loss: 0.19740 - acc: 0.9794 -- iter: 16/19\n",
      "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.17892\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 1000 | loss: 0.17892 - acc: 0.9815 -- iter: 19/19\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Prabhat\\Desktop\\100DaysOfMLCode\\40. Tensorflow Text Classification\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "farewell\n",
      "time\n",
      "age\n"
     ]
    }
   ],
   "source": [
    "# let's test the mdodel for a few sentences:\n",
    "# the first two sentences are used for training, and the last two sentences are not present in the training data.\n",
    "sent_1 = \"what time is it?\"\n",
    "sent_2 = \"I gotta go now\"\n",
    "sent_3 = \"do you know the time now?\"\n",
    "sent_4 = \"you must be a couple of years older then her!\"\n",
    "\n",
    "# a method that takes in a sentence and list of all words\n",
    "# and returns the data in a form the can be fed to tensorflow\n",
    "\n",
    "\n",
    "def get_tf_record(sentence):\n",
    "    global words\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    # bag of words\n",
    "    bow = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bow[i] = 1\n",
    "\n",
    "    return(np.array(bow))\n",
    "\n",
    "\n",
    "# we can start to predict the results for each of the 4 sentences\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_1)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_2)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_3)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_4)]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: https://sourcedexter.com/tensorflow-text-classification-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
