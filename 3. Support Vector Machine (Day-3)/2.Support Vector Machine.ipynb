{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are a very popular type of machine learning model used for classification when you have a small dataset.\n",
    "\n",
    "Defination From Analyticsvidya-\n",
    "\n",
    "“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well (look at the below snapshot).\n",
    "\n",
    "<img src=\"img/SVM.png\">\n",
    "\n",
    "Support Vectors are simply the co-ordinates of individual observation. Support Vector Machine is a frontier which best segregates the two classes (hyper-plane/ line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a support vector machine to classify two classes of Data and the way we're going to optimize The support vector machine this type of machine learning model is to use gradient descent .\n",
    "\n",
    "So that's what we're going to do and this is what it looks like\n",
    "\n",
    "<img src=\"img/1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's it looks like this so say was about two classes\n",
    "We have one class that's going to be denoted by blue dots, and the other class is going to be denoted by green dots\n",
    "So if we were plot both classes on a 2D graph ,an xy graph then we could draw a line a ,Decision boundary that best separates both of these classes and that line is called a hyperplane and our support vector machine helps us create it .\n",
    "\n",
    "\n",
    "We're going to talk about all the details of how this thing works\n",
    "\n",
    "But this is at a high level what it looks like and we're going to build it with just numpy and matplotlib , no tensorflow and all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What will we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a Support Vector Machine that will find the optimal hyperplane that maximizes the margin between two toy data classes using gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are some use cases for SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification, regression (time series prediction, etc) , outlier detection, clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is one which we're going to do and that is in fact the main case .\n",
    "That SVMs are used for that is the most popular case, but they can also be used for other types of machine learning problems like regression ,that is if we have some set of data points, and we're trying to predict the next point in that set of data so\n",
    "because stock prediction would be a good example.\n",
    "\n",
    "But we are going to do supervised classification that is when our data has labels, right?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to learn the mapping between the labels and the data and if we learn the mapping. that is a function .The function\n",
    "represents the mapping the relationship between these variables .If we learn this function then our machine learning model had done its job then we can use this function to plug in some new input data, and it's going to output the prediction right .\n",
    "So let's look at it. So in this example. We're going to use toy data right because it's more about the math and the algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does an SVM compare to other ML algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does this thing compare to the other machine learning algorithms?\n",
    "And there are so many of them there are random forest ,there are neural networks and there are others.\n",
    "\n",
    "list\n",
    "\n",
    "\n",
    "As a rule of thumb, SVMs are great for relatively small data sets with fewer outliers.\n",
    "\n",
    "\n",
    "Other algorithms (Random forests, deep neural networks, etc.) require more data but almost always come up with very robust models.\n",
    "\n",
    "The decision of which classifier to use depends on your dataset and the general complexity of the problem.\n",
    "\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Support Vector Machine ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a supervised machine learning algorithm which can be used for both classification or regression problems. But it's usually used for classification. Given 2 or more labeled classes of data, it acts as a discriminative classifier, formally defined by an optimal hyperplane that seperates all the classes. New examples that are then mapped into that same space can then be categorized based on on which side of the gap they fall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Support Vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set, they are what help us build our SVM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is support vector machine, so this thing can be used for both classification? \n",
    "Given two or more labelled classes of Data(we are using supervised learning it) it can create a discriminative classifiers -that is a classifier that can discriminate between Different classes .\n",
    "And the opposite to discriminative by the way is generative where we generate new Data. We take some training data\n",
    "Variate and variate  it in some way using our model, and that output is very similar to the training data. \n",
    "But it's novel data, but that's for later on .Anyway, so the way we build this hyperplane, and we'll talk about that term\n",
    "But the way we build this hyperplane or line this decision boundary between the classes is by maximizing the MarginThat is the space between that line and both of those classes.\n",
    "\n",
    "What do I mean by that?\n",
    "\n",
    "when I say both of those classes what I actually mean are the points in each check out the image are the\n",
    "Points in each of those classes that are closest to the decision Boundary and these points are called support\n",
    "Vectors.\n",
    "They are data point vectors that support the creation of this hyperplane that our support vector machine will create.\n",
    "Right so we are maximizing the Margin and why do we do that because we want to draw a line that is in the absolute\n",
    "perfectly the perfect middle spot between both of these sets of Data  .so that when we plot a new data point if\n",
    "it is of a certain class it will have the maximum Likelihood of falling on that side of the decision boundary.\n",
    "where it should and the only way to do that to maximize the space with which a\n",
    "New Data Point can fall into its correct class category is to maximize the space between Data points and put a line right in the middle of that space you see what I'm saying ,so small Margin we're maximizing the Margin and we're trying to draw a Decision boundary line of best Classification between both of those and we call it line a hyperplane, okay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whats a hyperplane?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/4.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well a hyperplane is a decision surface. So let's say our data is n  Dimensional where n is the number of features that you have.\n",
    "\n",
    "\n",
    "Hyperplane is N minus 1 dimensions .So if you have a two-dimensional graph where just like this .This R symbol with the two?\n",
    "Exponent Denotes a two-dimensional graph a hyperplane would then be 2 minus 1 right n minus 1 to one dimension?\n",
    "So would be a line, right?\n",
    "But if you we are in three dimensional space or our to three then a hyperplane is going to be two-dimensional because it's 3 minus 1 which is 2.\n",
    "Right so we have a plane and so you we can extrapolate this to many dimensions\n",
    "\n",
    "So if we had a 400 dimensional space which we often do in machine learning our data doesn't just have 2 or 3\n",
    "Features it has many many features right it's not so neatly packaged for us to visualize and that's where techniques like\n",
    "dimensionality reduction and all these come into play which we'll talk about later .\n",
    "\n",
    "\n",
    "As our machine is able to draw this Decision Boundary of N minus 1 dimensions then Given some new Data point if we put it into that ,model that functions that functions ,If we put it into that function if we plug it in and it's going to output the correct class of whatever it is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear vs nonlinear classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes our data is linearly seperable. That means for N classes with M features, we can learn a mapping that is a linear combination. (like y = mx + b). Or even a multidimensional hyperplane (y = x + z + b + q). No matter how many dimensions/features a set of classes have, we can represent the mapping using a linear function.\n",
    "\n",
    "But sometimes its not. Like if there was a quadratic mapping. Luckily for us SVMs can can efficiently perform a non-linear classification using what is called the kernel trick. I'll talk about that later in this blog.\n",
    "\n",
    "<img src=\"img/5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So no matter what model you're using a random forest a support vector machine a deep neural network in the end\n",
    "We are approximating we are guessing ,iteratively close we are getting\n",
    "Educated guess and rising of a different word for approximation, but we are approximating a function, right?\n",
    "We are trying to find what if the what is that optimal function and that function?\n",
    "Represents the relationship between all the variables in our data right that function  is that relationship?\n",
    "It's that mapping and if we can find that function\n",
    "Then we have learned ,We have learned from our data and so every machine learning model under the hood is just a function that we are trying to approximate and its coefficients are its weights and they are being updated over time through some optimization technique be that gradient descent or other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Build The SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To help us perform math operations\n",
    "import numpy as np\n",
    "#to plot our data and model visually\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#Step 1 - Define our data\n",
    "\n",
    "#Input data - Of the form [X value, Y value, Bias term]\n",
    "\n",
    "X = np.array([\n",
    "    [-2,4,-1],\n",
    "    [4,1,-1],\n",
    "    [1, 6, -1],\n",
    "    [2, 4, -1],\n",
    "    [6, 2, -1],\n",
    "\n",
    "])\n",
    "\n",
    "#Associated output labels - First 2 examples are labeled '-1' and last 3 are labeled '+1'\n",
    "y = np.array([-1,-1,1,1,1])\n",
    "\n",
    "#lets plot these examples on a 2D graph!\n",
    "#for each example\n",
    "for d, sample in enumerate(X):\n",
    "    # Plot the negative samples (the first 2)\n",
    "    if d < 2:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='_', linewidths=2)\n",
    "    # Plot the positive samples (the last 3)\n",
    "    else:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='+', linewidths=2)\n",
    "\n",
    "# Print a possible hyperplane, that is seperating the two classes.\n",
    "#we'll two points and draw the line between them (naive guess)\n",
    "plt.plot([-2,6],[6,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define our loss function (what to minimize) and our objective function (what to optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function\n",
    "\n",
    "We'll use the Hinge loss. This is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs).\n",
    "\n",
    "<img src=\"img/6.png\">\n",
    "\n",
    "c is the loss function, x the sample, y is the true label, f(x) the predicted label.\n",
    "\n",
    "<img src=\"img/7.png\">\n",
    "\n",
    "\n",
    "Objective Function\n",
    "\n",
    "<img src=\"img/8.png\">\n",
    "\n",
    "As you can see, our objective of a SVM consists of two terms. The first term is a regularizer, the heart of the SVM, the second term the loss. The regularizer balances between margin maximization and loss. We want to find the decision surface that is maximally far away from any data points.\n",
    "\n",
    "How do we minimize our loss/optimize for our objective (i.e learn)?\n",
    "\n",
    "We have to derive our objective function to get the gradients! Gradient descent ftw. As we have two terms, we will derive them seperately using the sum rule in differentiation.\n",
    "\n",
    "<img src=\"img/9.png\">\n",
    "\n",
    "\n",
    "This means, if we have a misclassified sample, we update the weight vector w using the gradients of both terms, else if classified correctly,we just update w by the gradient of the regularizer.\n",
    "\n",
    "Misclassification condition\n",
    "\n",
    "<img src=\"img/10.png\">\n",
    "\n",
    "Update rule for our weights (misclassified)\n",
    "\n",
    "<img src=\"img/11.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "including the learning rate η and the regularizer λ The learning rate is the length of the steps the algorithm makes down the gradient on the error curve.\n",
    "\n",
    "*Learning rate too high? The algorithm might overshoot the optimal point.\n",
    "*Learning rate too low? Could take too long to converge. Or never converge.\n",
    "\n",
    "The regularizer controls the trade off between the achieving a low training error and a low testing error that is the ability to generalize your classifier to unseen data. As a regulizing parameter we choose 1/epochs, so this parameter will decrease, as the number of epochs increases.\n",
    "\n",
    "*Regularizer too high? overfit (large testing error)\n",
    "*Regularizer too low? underfit (large training error)\n",
    "*Update rule for our weights (correctly classified)\n",
    "\n",
    "<img src=\"img/12.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets perform stochastic gradient descent to learn the seperating hyperplane between both classes\n",
    "\n",
    "def svm_sgd_plot(X, Y):\n",
    "    #Initialize our SVMs weight vector with zeros (3 values)\n",
    "    w = np.zeros(len(X[0]))\n",
    "    #The learning rate\n",
    "    eta = 1\n",
    "    #how many iterations to train for\n",
    "    epochs = 100000\n",
    "    #store misclassifications so we can plot how they change over time\n",
    "    errors = []\n",
    "\n",
    "    #training part, gradient descent part\n",
    "    for epoch in range(1,epochs):\n",
    "        error = 0\n",
    "        for i, x in enumerate(X):\n",
    "            #misclassification\n",
    "            if (Y[i]*np.dot(X[i], w)) < 1:\n",
    "                #misclassified update for ours weights\n",
    "                w = w + eta * ( (X[i] * Y[i]) + (-2  *(1/epoch)* w) )\n",
    "                error = 1\n",
    "            else:\n",
    "                #correct classification, update our weights\n",
    "                w = w + eta * (-2  *(1/epoch)* w)\n",
    "        errors.append(error)\n",
    "        \n",
    "\n",
    "    #lets plot the rate of classification errors during training for our SVM\n",
    "    plt.plot(errors, '|')\n",
    "    plt.ylim(0.5,1.5)\n",
    "    plt.axes().set_yticklabels([])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Misclassified')\n",
    "    plt.show()\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for d, sample in enumerate(X):\n",
    "    # Plot the negative samples\n",
    "    if d < 2:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='_', linewidths=2)\n",
    "    # Plot the positive samples\n",
    "    else:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='+', linewidths=2)\n",
    "\n",
    "# Add our test samples\n",
    "plt.scatter(2,2, s=120, marker='_', linewidths=2, color='yellow')\n",
    "plt.scatter(4,3, s=120, marker='+', linewidths=2, color='blue')\n",
    "\n",
    "\n",
    "\n",
    "# Print the hyperplane calculated by svm_sgd()\n",
    "x2=[w[0],w[1],-w[1],w[0]]\n",
    "x3=[w[0],w[1],w[1],-w[0]]\n",
    "\n",
    "\n",
    "\n",
    "x2x3 =np.array([x2,x3])\n",
    "X,Y,U,V = zip(*x2x3)\n",
    "ax = plt.gca()\n",
    "ax.quiver(X,Y,U,V,scale=1, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
